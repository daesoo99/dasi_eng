/**
 * SpeechService - STT, TTS, LLM ìŒì„± ì²˜ë¦¬ í†µí•© ì„œë¹„ìŠ¤
 * TypeScript ë³€í™˜: GoogleGenerativeAI + ìºì‹œ + í ê´€ë¦¬
 */

import { GoogleGenerativeAI } from "@google/generative-ai";
import { db } from "../config/firebase";
import { sttQueue, llmQueue, ttsQueue } from './taskQueue';
import { getCachedTTS, setCachedTTS } from './ttsCache';

// íƒ€ì… ì •ì˜
export interface SpeechEvaluation {
  majorError: boolean;
  naturalScore: number;
  correction: string;
}

export interface TTSResult {
  url: string;
  duration: number;
  voice: string;
  text: string;
  createdAt: string;
}

export interface LLMResponse {
  reply: string;
  preview: string;
}

export interface FullPipelineInput {
  audioBlob: Blob | string;
  voice: string;
  prompt?: string;
}

export interface FullPipelineResult {
  text: string;
  llm: LLMResponse;
  audio: TTSResult;
}

// Gemini AI ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ ì²´í¬)
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
let genAI: GoogleGenerativeAI | null = null;

if (GEMINI_API_KEY) {
  genAI = new GoogleGenerativeAI(GEMINI_API_KEY);
} else {
  console.warn('âš ï¸ GEMINI_API_KEY not found - using mock responses');
}

/**
 * STT (Speech-to-Text) ì²˜ë¦¬
 */
async function runSTT(audioBlob: Blob | string): Promise<string> {
  // Google Cloud Speech-to-Text API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„)
  console.log('ğŸ¤ Running STT...');
  
  // ê°œë°œ í™˜ê²½ì—ì„œëŠ” ëª¨ì˜ ì‘ë‹µ
  if (process.env.NODE_ENV === 'development') {
    await new Promise(resolve => setTimeout(resolve, 500));
    return 'Hello, this is a sample transcription';
  }
  
  // ì‹¤ì œ Google Cloud STT API í˜¸ì¶œ
  try {
    // const speech = require('@google-cloud/speech');
    // const client = new speech.SpeechClient();
    // const audio = { content: audioBlob };
    // const config = { encoding: 'WEBM_OPUS', sampleRateHertz: 16000, languageCode: 'en-US' };
    // const [response] = await client.recognize({ audio, config });
    // return response.results?.[0]?.alternatives?.[0]?.transcript || '';
    
    throw new Error('Google Cloud STT API ì„¤ì • í•„ìš”');
  } catch (error) {
    console.error('STT API í˜¸ì¶œ ì‹¤íŒ¨:', error);
    throw new Error('ìŒì„± ì¸ì‹ ì‹¤íŒ¨');
  }
}

/**
 * LLM (ëŒ€í™”í˜• AI) ì²˜ë¦¬
 */
async function runLLM(prompt: string): Promise<LLMResponse> {
  console.log('ğŸ¤– Running LLM...');
  
  if (genAI) {
    try {
      const model = genAI.getGenerativeModel({ model: "gemini-pro" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text = response.text();
      
      return {
        reply: text,
        preview: text.slice(0, 100) + (text.length > 100 ? '...' : '')
      };
    } catch (error) {
      console.error('Gemini API error:', error);
      // Fallback to mock
    }
  }
  
  // Mock response when API unavailable
  await new Promise(resolve => setTimeout(resolve, 1500));
  return {
    reply: 'Sample LLM response for prompt: ' + prompt.slice(0, 50),
    preview: 'Sample preview'
  };
}

/**
 * TTS ì‹¤ì œ í•©ì„± (ìºì‹œ ì œì™¸)
 */
async function actuallySynthesize(text: string, voice: string): Promise<TTSResult> {
  console.log(`ğŸ”Š Synthesizing TTS: "${text.slice(0, 30)}..." (${voice})`);
  
  // Google Cloud Text-to-Speech API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„)
  if (process.env.NODE_ENV === 'development') {
    await new Promise(resolve => setTimeout(resolve, 500));
    return {
      url: `https://mock-tts.dasi-english.com/${Date.now()}.mp3`,
      duration: Math.floor(text.length / 10) + 1,
      voice: voice,
      text: text,
      createdAt: new Date().toISOString()
    };
  }
  
  // ì‹¤ì œ Google Cloud TTS API í˜¸ì¶œ
  try {
    // const textToSpeech = require('@google-cloud/text-to-speech');
    // const client = new textToSpeech.TextToSpeechClient();
    // const request = {
    //   input: { text: text },
    //   voice: { languageCode: voice === 'female' ? 'en-US' : 'en-GB', ssmlGender: voice === 'female' ? 'FEMALE' : 'MALE' },
    //   audioConfig: { audioEncoding: 'MP3' }
    // };
    // const [response] = await client.synthesizeSpeech(request);
    // const audioBase64 = response.audioContent.toString('base64');
    // return { url: `data:audio/mp3;base64,${audioBase64}`, duration: Math.floor(text.length / 10) + 1, voice, text, createdAt: new Date().toISOString() };
    
    throw new Error('Google Cloud TTS API ì„¤ì • í•„ìš”');
  } catch (error) {
    console.error('TTS API í˜¸ì¶œ ì‹¤íŒ¨:', error);
    throw new Error('ìŒì„± í•©ì„± ì‹¤íŒ¨');
  }
}

/**
 * TTS (Text-to-Speech) ì²˜ë¦¬ (ìºì‹œ í¬í•¨)
 */
async function runTTS(text: string, voice: string): Promise<TTSResult> {
  // ìºì‹œ í™•ì¸
  const cached = await getCachedTTS(text, voice);
  if (cached) {
    console.log('ğŸ¯ TTS Cache Hit');
    return cached;
  }
  
  // ìºì‹œ ë¯¸ìŠ¤ ì‹œ ì‹¤ì œ TTS ì²˜ë¦¬
  const fresh = await actuallySynthesize(text, voice);
  setCachedTTS(text, voice, fresh);
  return fresh;
}

/**
 * ì‚¬ìš©ì ë°œí™” í‰ê°€ ë° í”¼ë“œë°± ìƒì„±
 */
export const evaluateSpeech = async (
  transcript: string, 
  targetPattern: string
): Promise<SpeechEvaluation> => {
  return llmQueue.add(async () => {
    console.log(`ğŸ“Š Evaluating speech: "${transcript}" vs target: "${targetPattern}"`);

    if (genAI) {
      try {
        const model = genAI.getGenerativeModel({ model: "gemini-pro" });
        const prompt = `Given the target pattern "${targetPattern}" and the user's speech "${transcript}", evaluate the speech for grammatical errors, pronunciation issues, and naturalness. Return JSON with: {"majorError": boolean, "naturalScore": number (0-100), "correction": string}`;

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const text = response.text();

        const evaluation = JSON.parse(text);
        return evaluation;
      } catch (error) {
        console.error("Gemini API evaluation error:", error);
      }
    }

    // Fallback ë”ë¯¸ í‰ê°€
    const dummyEvaluation: SpeechEvaluation = {
      majorError: transcript.toLowerCase().includes('error'),
      naturalScore: Math.floor(Math.random() * 40) + 60,
      correction: transcript.toLowerCase().includes('error') 
        ? transcript.replace(/error/gi, 'correction')
        : transcript,
    };

    return dummyEvaluation;
  });
};

/**
 * ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (STT â†’ LLM â†’ TTS)
 */
export const fullPipeline = async ({
  audioBlob,
  voice,
  prompt
}: FullPipelineInput): Promise<FullPipelineResult> => {
  const text = await sttQueue.add(() => runSTT(audioBlob));
  const llm = await llmQueue.add(() => runLLM(prompt ?? text));
  const audio = await ttsQueue.add(() => runTTS(llm.reply, voice));
  
  return { text, llm, audio };
};

// Export functions
export {
  runSTT,
  runLLM,
  runTTS
};